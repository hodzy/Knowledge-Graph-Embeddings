{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "KGE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFrRxGaVApdS"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "from operator import itemgetter\n",
        "import random\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import linalg as LA"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qTFi0ZkApdT"
      },
      "source": [
        "# maybe use named_tuples to represent the (h,r,t,h',t')\n",
        "if torch.cuda.is_available():  \n",
        "  device = \"cuda:0\" \n",
        "else:  \n",
        "  device = \"cpu\"\n",
        "# device"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g7OJ8XpApdV"
      },
      "source": [
        "def read_file(path, header, seperator):\n",
        "  return pd.read_csv(path, header= header, sep=seperator)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoXqpdwrApdX"
      },
      "source": [
        "# chose the dimension to be d=1000\n",
        "def convert_to_dict(text_data):\n",
        "  '''\n",
        "  convert read file from csv to dict, where key is the data read from the csv file and the value is a tensor of size 1000 (maybe later make size as a function parameter)\n",
        "  '''\n",
        "  output_dict = dict()\n",
        "#     torch.split(temp1, 1, dim=1)    \n",
        "  matrix=torch.rand(1000, len(text_data))\n",
        "  vectors = torch.split(matrix, 1, dim=1)\n",
        "    \n",
        "  for i in range(len(text_data)):\n",
        "#         temp = torch.reshape(vectors[i][0], (-1, 1))\n",
        "      output_dict[text_data[i]] = vectors[i]/torch.linalg.vector_norm(vectors[i])\n",
        "      output_dict[text_data[i]].requires_grad=True\n",
        "  return output_dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWgmi0eWApdW"
      },
      "source": [
        "def read_training_data(path, header, seperator):\n",
        "    '''\n",
        "    returns a list of sorted (according to relation) training instances (triplets) \n",
        "    i decided to return a sorted list, because i believe it improves the effeciency while generating the false samples\n",
        "    '''\n",
        "    data = pd.read_csv(path, header=header, sep=seperator)\n",
        "    training_data = data.values.tolist()\n",
        "    training_data = sorted(training_data, key=itemgetter(1)) \n",
        "    return training_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_dq8bWcApdU"
      },
      "source": [
        "def is_relation_false(training_data, sample):\n",
        "  '''\n",
        "  assuming a closed world world assumption\n",
        "  check if the false sample (sample) exist in the training data (meaning it is a true relation)\n",
        "  '''\n",
        "  for i in range(len(training_data)):\n",
        "      unique = True\n",
        "      if sample[0] == training_data[i][0] and sample[1] == training_data[i][2]:\n",
        "          unique = False\n",
        "          break\n",
        "                    \n",
        "      if training_data[i][1] != training_data[0][1]:\n",
        "          break\n",
        "  return unique"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OwdQOXkApdU"
      },
      "source": [
        "def is_relation_in_samples(generated_samples, sample_to_test):\n",
        "  '''\n",
        "  check if generated false sample was already generated in a pervious iteration (to avoid repetitions of false samples)\n",
        "  '''\n",
        "  generated = False\n",
        "  for sample in generated_samples:\n",
        "      if sample_to_test[0] == sample[0] and sample_to_test[1] == sample[1]:\n",
        "          generated = True\n",
        "  return generated"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqLOIpqoApdU"
      },
      "source": [
        "def generate_false_samples(entities_dict, positive_sample, training_data, index_start_of_relation, num_needed_samples):\n",
        "  '''\n",
        "  positive_sample: represents the sample we want to generate the false samples for,  form: [h, r, t]\n",
        "  \n",
        "  training_data: all the training data sorted according to the relation (h, r, t)\n",
        "  \n",
        "  index_start_of_relation: represents the index in the training_data, where the samples in the training set has the same relation (r) as the relation in the positive sample\n",
        "\n",
        "  num_needed_samples: how many false smaples to generate\n",
        "  '''\n",
        "  counter = 0  # to avoid running in an infinite loop, if there are not enough false relations between the entities to generate the false samples needed\n",
        "  generated_samples = []  # generated false samples\n",
        "\n",
        "  while len(generated_samples) < num_needed_samples and counter < 30000:\n",
        "\n",
        "      head_or_tail = random.randint(0, 1)  # do decide whether to replace the head or the tail in the false sample\n",
        "      random_entity = random.choice(entities_dict)  # choose a random entity\n",
        "      counter += 1\n",
        "        \n",
        "        # 1 for head\n",
        "      if(head_or_tail):\n",
        "          negative_sample = [random_entity, positive_sample[2]]    \n",
        "      else:\n",
        "          negative_sample = [positive_sample[0], random_entity]\n",
        "\n",
        "      relation_is_false = is_relation_false(training_data[index_start_of_relation:], negative_sample)\n",
        "      relation_already_generated = is_relation_in_samples(generated_samples, negative_sample)\n",
        "            \n",
        "      if is_relation_false and not(relation_already_generated):\n",
        "          generated_samples.append(negative_sample)\n",
        "    \n",
        "  positive_sample.extend(generated_samples)  # [h, r, t, [generated_sample_1], [generated_sample_2], ... [generated_sample_n]]\n",
        "  return positive_sample  \n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oJB7rv9ApdX"
      },
      "source": [
        "def is_false_samples_generated():\n",
        "  '''\n",
        "  checks if training data with false samples inclded were generated and saved in a previous run of the program\n",
        "  '''\n",
        "  extended_data = Path(\"/content/extended_training_data.txt\")\n",
        "  return extended_data.is_file()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj3_1rTNApdY"
      },
      "source": [
        "# def encoding_training_data(entity_dict, relation_dict, training_data):\n",
        "#     for i in range(len(training_data)):\n",
        "#         for j in range(len(training_data[i])):\n",
        "#             if j == 1:\n",
        "#                 training_data[i][j] = relation_dict[training_data[i][j]]\n",
        "#             elif j < 3:\n",
        "#                 training_data[i][j] = entity_dict[training_data[i][j]]\n",
        "#             else:\n",
        "#                 training_data[i][j] = training_data[i][j].strip(\"[]\").split(\",\")\n",
        "#                 training_data[i][j] = [entity_dict[int(training_data[i][j][0])], entity_dict[int(training_data[i][j][1])]]\n",
        "#     return training_data"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ThsOLEApdY"
      },
      "source": [
        "def read_extended_training_data(training_data):\n",
        "  '''\n",
        "  gets text file of extended training data (training data that includes false samples)\n",
        "  and returns a list of training samples in the form [h, r, t, [generated_sample_1], [generated_sample_2], ... [generated_sample_n]]\n",
        "  '''\n",
        "  for i in range(len(training_data)):\n",
        "      for j in range(3, len(training_data[i])):\n",
        "          training_data[i][j] = training_data[i][j].strip(\"[]\").split(\",\")\n",
        "          training_data[i][j][0] = int(training_data[i][j][0]) \n",
        "          training_data[i][j][1] = int(training_data[i][j][1])\n",
        "  return training_data"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugJyKKjfApdd"
      },
      "source": [
        "def reshape_training_data(training_data):\n",
        "  '''\n",
        "  reshapes training data samples to the form\n",
        "  [h, r, t, [false_heads], [false_tails]]\n",
        "  '''\n",
        "  for i in range(len(training_data)):\n",
        "    sample = []\n",
        "    new_training_ex_1 =[]\n",
        "    new_training_ex_2 =[]\n",
        "    sample.extend(training_data[i][0:3])\n",
        "    for j in range(3, len(training_data[i])):\n",
        "      new_training_ex_1.append(training_data[i][j][0])\n",
        "      new_training_ex_2.append(training_data[i][j][1])\n",
        "    sample.append(new_training_ex_1)\n",
        "    sample.append(new_training_ex_2)\n",
        "    training_data[i] = sample\n",
        "  return training_data\n",
        "# print(sample)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcfZLHfvApda"
      },
      "source": [
        "entities_temp_list = read_file('/content/entities.dict', header=None, seperator='\\t')[1]\n",
        "entities = convert_to_dict(entities_temp_list)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2TTrNgRApdb"
      },
      "source": [
        "relation_temp_list = read_file('/content/relations.dict', header=None, seperator='\\t')[1]\n",
        "relations = convert_to_dict(relation_temp_list)\n",
        "# relations"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7I_Pl9NApdb"
      },
      "source": [
        "# # training_data = read_training_data('./data/wn18rr/train.txt', header=None, seperator='\\t')\n",
        "# training_data = read_training_data('./content/train.txt', header=None, seperator='\\t')\n",
        "# training_data[0:10]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8ivceb8Apdc",
        "outputId": "954a5e44-43f6-4926-8c04-ec1052f4edc1"
      },
      "source": [
        "# oprimaly false_samples_count should be 20, so num_needed_samples= 20\n",
        "if is_false_samples_generated():\n",
        "    print(\"found file\")\n",
        "    read_training_data = pd.read_csv('/content/extended_training_data.txt', header=None, sep='\\t')\n",
        "    training_data = read_training_data.values.tolist()\n",
        "    training_data = read_extended_training_data(training_data)\n",
        "\n",
        "    # should add an if statement to check shape\n",
        "    training_data = reshape_training_data(training_data)\n",
        "else:\n",
        "\n",
        "  # training_data = read_training_data('./data/wn18rr/train.txt', header=None, seperator='\\t')\n",
        "  training_data = read_training_data('./content/train.txt', header=None, seperator='\\t')\n",
        "  training_data[0:10]\n",
        "  for i  in range(len(training_data)):\n",
        "      print(i)\n",
        "      if not (training_data[i][1] == training_data[i-1][1]):\n",
        "          index_start_of_relation = i\n",
        "          print(\"new index: \", index_start_of_relation)\n",
        "          print(training_data[i][1])\n",
        "                        \n",
        "      training_data[i] = generate_false_samples(entities_temp_list, \n",
        "                                              training_data[i], \n",
        "                                              training_data, \n",
        "                                              index_start_of_relation,\n",
        "                                              num_needed_samples=10)\n",
        "    \n",
        "  textfile = open(\"/content/extended_training_data.txt\", \"w\")\n",
        "  for row in training_data:\n",
        "      for i in range(len(row)):\n",
        "          if i != len(row)-1:\n",
        "              textfile.write(str(row[i]) + \"\\t\")\n",
        "          else:\n",
        "              textfile.write(str(row[i]))\n",
        "      textfile.write(\"\\n\")\n",
        "  textfile.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSLGpOupApdc"
      },
      "source": [
        "# check if any of training_data samples has a different len \n",
        "# ex. = 13 [h, r, t, [generated_sample_1], [generated_sample_2], ... [generated_sample_10]] \n",
        "\n",
        "# counter = 0\n",
        "# for i in training_data:\n",
        "#     if len(i) != 13:\n",
        "#         counter +=1\n",
        "# print(counter)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcYDHNWSO5YP"
      },
      "source": [
        "# difference = []\n",
        "# ex_1 = [torch.tensor([2,2,3,4]), torch.tensor([1,2,3,4])]\n",
        "# ex_2 = [torch.tensor([3,2,3,4]), torch.tensor([1,2,3,4])]\n",
        "# zip_object = zip(sample[4], sample[5])\n",
        "# for head, tail in zip_object:\n",
        "#     difference.append(head-tail)\n",
        "# print(difference)\n",
        "# # print([tensy for tensy in ex_2])\n",
        "# # print([(torch.subtract(tensy, ex_1)) for tensy in ex_2])\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZC8M7hDlDvO"
      },
      "source": [
        "class KGEDataset(Dataset):\n",
        "  def __init__(self, training_data):\n",
        "        # Initialize data, download, etc.\n",
        "        self.training_data = training_data\n",
        "        self.n_samples = len(training_data)\n",
        "\n",
        "\n",
        "  # support indexing such that dataset[i] can be used to get i-th sample\n",
        "  def __getitem__(self, index):\n",
        "    return training_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dII84aAWKYmr"
      },
      "source": [
        "class KGEmodel(nn.Module):\n",
        "  def __init__(self, entities, relations):\n",
        "    super(KGEmodel, self).__init__()\n",
        "    self.w = entities.copy()   # start with keys and values of entities\n",
        "    self.w.update(relations)    # modifies w with keys and values of relations\n",
        "\n",
        "    # ParameterDict didn't work for now (possibly try again later) (looks more elegant)\n",
        "    # self.entities = nn.ParameterDict(ent)\n",
        "    # self.relations = nn.ParameterDict(rel)\n",
        "  \n",
        "  def forward(self, mini_batch):\n",
        "    rearranged_samples = []\n",
        "    for i in range(len(mini_batch[0])):\n",
        "      sample=[]\n",
        "      for j in range(3):\n",
        "        # sample.append(mini_batch[j][0][i])\n",
        "        sample.append(mini_batch[j][i])\n",
        "\n",
        "      false_head=[]\n",
        "      false_tail=[]\n",
        "      for j in range(len(mini_batch[3])):\n",
        "        false_head.append(mini_batch[3][j][i])\n",
        "        false_tail.append(mini_batch[4][j][i])\n",
        "\n",
        "      sample.append(false_head)\n",
        "      sample.append(false_tail)\n",
        "      rearranged_samples.append(sample)\n",
        "    result = []\n",
        "    for sample in rearranged_samples:\n",
        "      encoded_sample = []\n",
        "      encoded_sample.append(self.w[int(sample[0])].to(device))\n",
        "      encoded_sample.append(self.w[str(sample[1])].to(device))\n",
        "      encoded_sample.append(self.w[int(sample[2])].to(device))\n",
        "      false_h_encoded = []\n",
        "      false_t_encoded = []\n",
        "      for head in sample[3]:\n",
        "        false_h_encoded.append(self.w[int(head)].to(device))\n",
        "    \n",
        "      for tail in sample[4]:\n",
        "        false_t_encoded.append(self.w[int(tail)].to(device))\n",
        "    \n",
        "      encoded_sample.append(false_h_encoded)\n",
        "      encoded_sample.append(false_t_encoded)\n",
        "\n",
        "      result.append(encoded_sample)\n",
        "    return result \n",
        "      "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLuO6B6_ZNvM"
      },
      "source": [
        "def loss_func_new(margin, mini_batch):\n",
        "  loss = 0\n",
        "  for sample in mini_batch:\n",
        "    positive_score = LA.norm(sample[0] + sample[1] - sample[2])\n",
        "    negative_scores = [LA.norm(head + sample[1] - tail) for head, tail in zip(sample[3], sample[4])]\n",
        "    \n",
        "    loss += sum([max(0, margin - positive_score + value) for value in negative_scores])\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mONxvLCfQrM"
      },
      "source": [
        "batch_size = 256\n",
        "\n",
        "model = KGEmodel(entities, relations)\n",
        "model= model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(list(model.w.values()), lr=0.01)\n",
        "\n",
        "dataset = KGEDataset(training_data)\n",
        "\n",
        "# margin = 1, dimension d = 20000\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFYbzO8aktcG"
      },
      "source": [
        "# data = iter(train_loader).next()\n",
        "# result = model(data)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JFKCxnVApde",
        "outputId": "713629b5-44b8-4744-ae40-6793c09ad2a8"
      },
      "source": [
        "# Training loop\n",
        "f = open(\"/content/training_loss.txt\", \"w\")\n",
        "\n",
        "\n",
        "num_epochs = 6\n",
        "total_samples = len(training_data)\n",
        "n_iterations = math.ceil(total_samples/batch_size)\n",
        "total_loss=0\n",
        "# print(total_samples, n_iterations)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      encoding = model(data)\n",
        "      loss = loss_func_new(0.1, encoding)\n",
        "\n",
        "      total_loss += loss\n",
        "\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if (i+1) % 10 == 0:\n",
        "        f.write(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_iterations}], Total Loss: {total_loss:.4f}, Loss: {loss:.4f} \\n')\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_iterations}], Total Loss: {total_loss:.4f}, Loss: {loss:.4f}')\n",
        "        total_loss = 0\n",
        "f.close()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/6], Step [10/340], Total Loss: 22.8005, Loss: 1.7635\n",
            "Epoch [1/6], Step [20/340], Total Loss: 24.3342, Loss: 1.6415\n",
            "Epoch [1/6], Step [30/340], Total Loss: 25.1003, Loss: 2.9208\n",
            "Epoch [1/6], Step [40/340], Total Loss: 22.1082, Loss: 2.0500\n",
            "Epoch [1/6], Step [50/340], Total Loss: 22.8062, Loss: 2.6120\n",
            "Epoch [1/6], Step [60/340], Total Loss: 22.1108, Loss: 1.9210\n",
            "Epoch [1/6], Step [70/340], Total Loss: 26.0997, Loss: 3.0515\n",
            "Epoch [1/6], Step [80/340], Total Loss: 22.2122, Loss: 1.8348\n",
            "Epoch [1/6], Step [90/340], Total Loss: 31.4027, Loss: 3.0062\n",
            "Epoch [1/6], Step [100/340], Total Loss: 31.4812, Loss: 2.7533\n",
            "Epoch [1/6], Step [110/340], Total Loss: 35.5111, Loss: 2.0464\n",
            "Epoch [1/6], Step [120/340], Total Loss: 27.2865, Loss: 2.4893\n",
            "Epoch [1/6], Step [130/340], Total Loss: 23.6646, Loss: 2.3906\n",
            "Epoch [1/6], Step [140/340], Total Loss: 26.5127, Loss: 2.2918\n",
            "Epoch [1/6], Step [150/340], Total Loss: 24.7136, Loss: 1.9691\n",
            "Epoch [1/6], Step [160/340], Total Loss: 25.3134, Loss: 2.1863\n",
            "Epoch [1/6], Step [170/340], Total Loss: 27.2696, Loss: 3.6236\n",
            "Epoch [1/6], Step [180/340], Total Loss: 26.7934, Loss: 2.5603\n",
            "Epoch [1/6], Step [190/340], Total Loss: 25.0692, Loss: 1.9594\n",
            "Epoch [1/6], Step [200/340], Total Loss: 30.4088, Loss: 2.4656\n",
            "Epoch [1/6], Step [210/340], Total Loss: 24.6097, Loss: 2.7528\n",
            "Epoch [1/6], Step [220/340], Total Loss: 25.3626, Loss: 2.7075\n",
            "Epoch [1/6], Step [230/340], Total Loss: 31.8549, Loss: 3.1144\n",
            "Epoch [1/6], Step [240/340], Total Loss: 25.6368, Loss: 2.3886\n",
            "Epoch [1/6], Step [250/340], Total Loss: 25.8603, Loss: 2.6572\n",
            "Epoch [1/6], Step [260/340], Total Loss: 31.8331, Loss: 2.2640\n",
            "Epoch [1/6], Step [270/340], Total Loss: 27.4987, Loss: 2.8983\n",
            "Epoch [1/6], Step [280/340], Total Loss: 25.4547, Loss: 2.0364\n",
            "Epoch [1/6], Step [290/340], Total Loss: 26.0226, Loss: 2.6326\n",
            "Epoch [1/6], Step [300/340], Total Loss: 28.0695, Loss: 2.5942\n",
            "Epoch [1/6], Step [310/340], Total Loss: 27.0529, Loss: 3.2240\n",
            "Epoch [1/6], Step [320/340], Total Loss: 27.6281, Loss: 3.2892\n",
            "Epoch [1/6], Step [330/340], Total Loss: 25.0911, Loss: 2.7603\n",
            "Epoch [1/6], Step [340/340], Total Loss: 23.7343, Loss: 0.3462\n",
            "Epoch [2/6], Step [10/340], Total Loss: 9.9367, Loss: 0.9224\n",
            "Epoch [2/6], Step [20/340], Total Loss: 9.7868, Loss: 1.4739\n",
            "Epoch [2/6], Step [30/340], Total Loss: 9.3230, Loss: 1.0642\n",
            "Epoch [2/6], Step [40/340], Total Loss: 9.9551, Loss: 0.8568\n",
            "Epoch [2/6], Step [50/340], Total Loss: 9.8435, Loss: 0.9587\n",
            "Epoch [2/6], Step [60/340], Total Loss: 16.8557, Loss: 0.9588\n",
            "Epoch [2/6], Step [70/340], Total Loss: 14.5864, Loss: 0.8238\n",
            "Epoch [2/6], Step [80/340], Total Loss: 9.6227, Loss: 0.6330\n",
            "Epoch [2/6], Step [90/340], Total Loss: 8.7623, Loss: 0.6965\n",
            "Epoch [2/6], Step [100/340], Total Loss: 9.4429, Loss: 0.7446\n",
            "Epoch [2/6], Step [110/340], Total Loss: 9.8839, Loss: 1.1453\n",
            "Epoch [2/6], Step [120/340], Total Loss: 9.2201, Loss: 1.3373\n",
            "Epoch [2/6], Step [130/340], Total Loss: 10.4178, Loss: 0.6878\n",
            "Epoch [2/6], Step [140/340], Total Loss: 12.5278, Loss: 1.9312\n",
            "Epoch [2/6], Step [150/340], Total Loss: 17.2359, Loss: 7.3972\n",
            "Epoch [2/6], Step [160/340], Total Loss: 16.0649, Loss: 0.8867\n",
            "Epoch [2/6], Step [170/340], Total Loss: 9.9938, Loss: 0.9118\n",
            "Epoch [2/6], Step [180/340], Total Loss: 9.9795, Loss: 0.7443\n",
            "Epoch [2/6], Step [190/340], Total Loss: 16.1249, Loss: 0.9123\n",
            "Epoch [2/6], Step [200/340], Total Loss: 16.4355, Loss: 0.9208\n",
            "Epoch [2/6], Step [210/340], Total Loss: 10.0720, Loss: 0.7431\n",
            "Epoch [2/6], Step [220/340], Total Loss: 9.8553, Loss: 1.2383\n",
            "Epoch [2/6], Step [230/340], Total Loss: 10.1350, Loss: 1.3112\n",
            "Epoch [2/6], Step [240/340], Total Loss: 10.2864, Loss: 0.8259\n",
            "Epoch [2/6], Step [250/340], Total Loss: 10.6569, Loss: 1.2603\n",
            "Epoch [2/6], Step [260/340], Total Loss: 11.3118, Loss: 1.2484\n",
            "Epoch [2/6], Step [270/340], Total Loss: 9.9829, Loss: 1.2264\n",
            "Epoch [2/6], Step [280/340], Total Loss: 10.5919, Loss: 1.0710\n",
            "Epoch [2/6], Step [290/340], Total Loss: 10.8487, Loss: 0.8964\n",
            "Epoch [2/6], Step [300/340], Total Loss: 11.2107, Loss: 1.3227\n",
            "Epoch [2/6], Step [310/340], Total Loss: 9.5130, Loss: 1.4781\n",
            "Epoch [2/6], Step [320/340], Total Loss: 19.0423, Loss: 1.1924\n",
            "Epoch [2/6], Step [330/340], Total Loss: 11.6041, Loss: 0.9594\n",
            "Epoch [2/6], Step [340/340], Total Loss: 11.2284, Loss: 0.1904\n",
            "Epoch [3/6], Step [10/340], Total Loss: 4.0251, Loss: 0.3814\n",
            "Epoch [3/6], Step [20/340], Total Loss: 10.4380, Loss: 0.6701\n",
            "Epoch [3/6], Step [30/340], Total Loss: 10.5843, Loss: 0.4144\n",
            "Epoch [3/6], Step [40/340], Total Loss: 5.2886, Loss: 0.6222\n",
            "Epoch [3/6], Step [50/340], Total Loss: 4.4573, Loss: 0.3401\n",
            "Epoch [3/6], Step [60/340], Total Loss: 4.2364, Loss: 0.4925\n",
            "Epoch [3/6], Step [70/340], Total Loss: 4.9762, Loss: 0.4905\n",
            "Epoch [3/6], Step [80/340], Total Loss: 4.3669, Loss: 0.3898\n",
            "Epoch [3/6], Step [90/340], Total Loss: 5.6138, Loss: 0.9968\n",
            "Epoch [3/6], Step [100/340], Total Loss: 5.3606, Loss: 0.3209\n",
            "Epoch [3/6], Step [110/340], Total Loss: 4.9925, Loss: 0.9686\n",
            "Epoch [3/6], Step [120/340], Total Loss: 4.8810, Loss: 0.6916\n",
            "Epoch [3/6], Step [130/340], Total Loss: 5.6176, Loss: 0.4152\n",
            "Epoch [3/6], Step [140/340], Total Loss: 11.5046, Loss: 0.4895\n",
            "Epoch [3/6], Step [150/340], Total Loss: 4.7878, Loss: 0.4684\n",
            "Epoch [3/6], Step [160/340], Total Loss: 11.2873, Loss: 0.4602\n",
            "Epoch [3/6], Step [170/340], Total Loss: 5.2749, Loss: 0.4691\n",
            "Epoch [3/6], Step [180/340], Total Loss: 10.8001, Loss: 0.5818\n",
            "Epoch [3/6], Step [190/340], Total Loss: 4.9992, Loss: 0.5251\n",
            "Epoch [3/6], Step [200/340], Total Loss: 4.7903, Loss: 0.3556\n",
            "Epoch [3/6], Step [210/340], Total Loss: 5.1062, Loss: 0.5191\n",
            "Epoch [3/6], Step [220/340], Total Loss: 5.0459, Loss: 0.5988\n",
            "Epoch [3/6], Step [230/340], Total Loss: 10.5846, Loss: 0.8230\n",
            "Epoch [3/6], Step [240/340], Total Loss: 5.8584, Loss: 0.5575\n",
            "Epoch [3/6], Step [250/340], Total Loss: 4.8948, Loss: 0.7803\n",
            "Epoch [3/6], Step [260/340], Total Loss: 5.3327, Loss: 0.7442\n",
            "Epoch [3/6], Step [270/340], Total Loss: 5.4754, Loss: 0.7149\n",
            "Epoch [3/6], Step [280/340], Total Loss: 5.0006, Loss: 0.3607\n",
            "Epoch [3/6], Step [290/340], Total Loss: 5.8828, Loss: 0.6347\n",
            "Epoch [3/6], Step [300/340], Total Loss: 6.4302, Loss: 0.7170\n",
            "Epoch [3/6], Step [310/340], Total Loss: 5.5948, Loss: 0.5207\n",
            "Epoch [3/6], Step [320/340], Total Loss: 5.2595, Loss: 0.4243\n",
            "Epoch [3/6], Step [330/340], Total Loss: 12.2486, Loss: 0.7520\n",
            "Epoch [3/6], Step [340/340], Total Loss: 5.3598, Loss: 0.0797\n",
            "Epoch [4/6], Step [10/340], Total Loss: 2.5128, Loss: 0.2951\n",
            "Epoch [4/6], Step [20/340], Total Loss: 8.1368, Loss: 0.2467\n",
            "Epoch [4/6], Step [30/340], Total Loss: 8.4104, Loss: 5.5822\n",
            "Epoch [4/6], Step [40/340], Total Loss: 7.9017, Loss: 4.9747\n",
            "Epoch [4/6], Step [50/340], Total Loss: 3.0485, Loss: 0.4497\n",
            "Epoch [4/6], Step [60/340], Total Loss: 8.3928, Loss: 0.3274\n",
            "Epoch [4/6], Step [70/340], Total Loss: 3.2571, Loss: 0.2204\n",
            "Epoch [4/6], Step [80/340], Total Loss: 3.5548, Loss: 0.4585\n",
            "Epoch [4/6], Step [90/340], Total Loss: 2.4699, Loss: 0.2966\n",
            "Epoch [4/6], Step [100/340], Total Loss: 13.2151, Loss: 0.2302\n",
            "Epoch [4/6], Step [110/340], Total Loss: 2.8234, Loss: 0.4474\n",
            "Epoch [4/6], Step [120/340], Total Loss: 2.4141, Loss: 0.1854\n",
            "Epoch [4/6], Step [130/340], Total Loss: 2.7342, Loss: 0.1721\n",
            "Epoch [4/6], Step [140/340], Total Loss: 3.3049, Loss: 0.4067\n",
            "Epoch [4/6], Step [150/340], Total Loss: 2.6602, Loss: 0.1474\n",
            "Epoch [4/6], Step [160/340], Total Loss: 8.7387, Loss: 0.1662\n",
            "Epoch [4/6], Step [170/340], Total Loss: 3.0369, Loss: 0.1780\n",
            "Epoch [4/6], Step [180/340], Total Loss: 3.1618, Loss: 0.2015\n",
            "Epoch [4/6], Step [190/340], Total Loss: 2.6774, Loss: 0.2285\n",
            "Epoch [4/6], Step [200/340], Total Loss: 2.8082, Loss: 0.3513\n",
            "Epoch [4/6], Step [210/340], Total Loss: 3.9891, Loss: 0.2684\n",
            "Epoch [4/6], Step [220/340], Total Loss: 3.3389, Loss: 0.3058\n",
            "Epoch [4/6], Step [230/340], Total Loss: 3.3841, Loss: 0.1348\n",
            "Epoch [4/6], Step [240/340], Total Loss: 2.9473, Loss: 0.4640\n",
            "Epoch [4/6], Step [250/340], Total Loss: 2.3532, Loss: 0.2715\n",
            "Epoch [4/6], Step [260/340], Total Loss: 3.3007, Loss: 0.4274\n",
            "Epoch [4/6], Step [270/340], Total Loss: 2.9394, Loss: 0.3517\n",
            "Epoch [4/6], Step [280/340], Total Loss: 3.0338, Loss: 0.3473\n",
            "Epoch [4/6], Step [290/340], Total Loss: 3.3894, Loss: 0.3995\n",
            "Epoch [4/6], Step [300/340], Total Loss: 3.4443, Loss: 0.1444\n",
            "Epoch [4/6], Step [310/340], Total Loss: 2.5984, Loss: 0.3948\n",
            "Epoch [4/6], Step [320/340], Total Loss: 3.5487, Loss: 0.1748\n",
            "Epoch [4/6], Step [330/340], Total Loss: 3.3342, Loss: 0.3482\n",
            "Epoch [4/6], Step [340/340], Total Loss: 3.4790, Loss: 0.0837\n",
            "Epoch [5/6], Step [10/340], Total Loss: 6.9120, Loss: 0.1054\n",
            "Epoch [5/6], Step [20/340], Total Loss: 8.2323, Loss: 0.2992\n",
            "Epoch [5/6], Step [30/340], Total Loss: 1.5362, Loss: 0.1874\n",
            "Epoch [5/6], Step [40/340], Total Loss: 1.9068, Loss: 0.3079\n",
            "Epoch [5/6], Step [50/340], Total Loss: 1.7983, Loss: 0.0823\n",
            "Epoch [5/6], Step [60/340], Total Loss: 1.8200, Loss: 0.2645\n",
            "Epoch [5/6], Step [70/340], Total Loss: 7.7272, Loss: 0.2172\n",
            "Epoch [5/6], Step [80/340], Total Loss: 1.6135, Loss: 0.2082\n",
            "Epoch [5/6], Step [90/340], Total Loss: 1.5999, Loss: 0.1213\n",
            "Epoch [5/6], Step [100/340], Total Loss: 1.8855, Loss: 0.1362\n",
            "Epoch [5/6], Step [110/340], Total Loss: 1.5506, Loss: 0.0616\n",
            "Epoch [5/6], Step [120/340], Total Loss: 1.8691, Loss: 0.1695\n",
            "Epoch [5/6], Step [130/340], Total Loss: 1.2847, Loss: 0.3051\n",
            "Epoch [5/6], Step [140/340], Total Loss: 1.6957, Loss: 0.1373\n",
            "Epoch [5/6], Step [150/340], Total Loss: 1.8845, Loss: 0.2186\n",
            "Epoch [5/6], Step [160/340], Total Loss: 2.3263, Loss: 0.3653\n",
            "Epoch [5/6], Step [170/340], Total Loss: 1.2525, Loss: 0.0781\n",
            "Epoch [5/6], Step [180/340], Total Loss: 1.7547, Loss: 0.2884\n",
            "Epoch [5/6], Step [190/340], Total Loss: 7.4297, Loss: 0.1988\n",
            "Epoch [5/6], Step [200/340], Total Loss: 1.9601, Loss: 0.1649\n",
            "Epoch [5/6], Step [210/340], Total Loss: 7.1750, Loss: 0.5931\n",
            "Epoch [5/6], Step [220/340], Total Loss: 1.8505, Loss: 0.2265\n",
            "Epoch [5/6], Step [230/340], Total Loss: 2.0815, Loss: 0.1019\n",
            "Epoch [5/6], Step [240/340], Total Loss: 1.7178, Loss: 0.2840\n",
            "Epoch [5/6], Step [250/340], Total Loss: 1.6583, Loss: 0.0622\n",
            "Epoch [5/6], Step [260/340], Total Loss: 1.8760, Loss: 0.1679\n",
            "Epoch [5/6], Step [270/340], Total Loss: 7.6194, Loss: 0.2081\n",
            "Epoch [5/6], Step [280/340], Total Loss: 1.8746, Loss: 0.3655\n",
            "Epoch [5/6], Step [290/340], Total Loss: 1.7546, Loss: 0.3235\n",
            "Epoch [5/6], Step [300/340], Total Loss: 2.2838, Loss: 0.1488\n",
            "Epoch [5/6], Step [310/340], Total Loss: 2.3296, Loss: 0.2369\n",
            "Epoch [5/6], Step [320/340], Total Loss: 8.1198, Loss: 0.1881\n",
            "Epoch [5/6], Step [330/340], Total Loss: 2.0139, Loss: 0.1208\n",
            "Epoch [5/6], Step [340/340], Total Loss: 2.6041, Loss: 0.0186\n",
            "Epoch [6/6], Step [10/340], Total Loss: 0.9819, Loss: 0.0143\n",
            "Epoch [6/6], Step [20/340], Total Loss: 0.9836, Loss: 0.0137\n",
            "Epoch [6/6], Step [30/340], Total Loss: 1.2441, Loss: 0.0646\n",
            "Epoch [6/6], Step [40/340], Total Loss: 0.6597, Loss: 0.0216\n",
            "Epoch [6/6], Step [50/340], Total Loss: 1.0800, Loss: 0.2230\n",
            "Epoch [6/6], Step [60/340], Total Loss: 1.3154, Loss: 0.1506\n",
            "Epoch [6/6], Step [70/340], Total Loss: 6.2544, Loss: 0.1326\n",
            "Epoch [6/6], Step [80/340], Total Loss: 1.4012, Loss: 0.1454\n",
            "Epoch [6/6], Step [90/340], Total Loss: 0.8407, Loss: 0.0338\n",
            "Epoch [6/6], Step [100/340], Total Loss: 1.2449, Loss: 0.1433\n",
            "Epoch [6/6], Step [110/340], Total Loss: 1.3374, Loss: 0.2110\n",
            "Epoch [6/6], Step [120/340], Total Loss: 0.8445, Loss: 0.0772\n",
            "Epoch [6/6], Step [130/340], Total Loss: 6.1893, Loss: 0.1546\n",
            "Epoch [6/6], Step [140/340], Total Loss: 1.1300, Loss: 0.0969\n",
            "Epoch [6/6], Step [150/340], Total Loss: 1.3476, Loss: 0.0710\n",
            "Epoch [6/6], Step [160/340], Total Loss: 6.4684, Loss: 0.0455\n",
            "Epoch [6/6], Step [170/340], Total Loss: 1.0078, Loss: 0.0498\n",
            "Epoch [6/6], Step [180/340], Total Loss: 1.3282, Loss: 0.0407\n",
            "Epoch [6/6], Step [190/340], Total Loss: 1.3378, Loss: 0.1020\n",
            "Epoch [6/6], Step [200/340], Total Loss: 1.4656, Loss: 0.0673\n",
            "Epoch [6/6], Step [210/340], Total Loss: 6.7462, Loss: 0.1422\n",
            "Epoch [6/6], Step [220/340], Total Loss: 15.6666, Loss: 4.6772\n",
            "Epoch [6/6], Step [230/340], Total Loss: 1.4318, Loss: 0.1046\n",
            "Epoch [6/6], Step [240/340], Total Loss: 2.0201, Loss: 0.0763\n",
            "Epoch [6/6], Step [250/340], Total Loss: 1.5380, Loss: 0.1744\n",
            "Epoch [6/6], Step [260/340], Total Loss: 1.3378, Loss: 0.1546\n",
            "Epoch [6/6], Step [270/340], Total Loss: 1.4103, Loss: 0.0788\n",
            "Epoch [6/6], Step [280/340], Total Loss: 2.0001, Loss: 0.1068\n",
            "Epoch [6/6], Step [290/340], Total Loss: 1.1316, Loss: 0.0459\n",
            "Epoch [6/6], Step [300/340], Total Loss: 1.4418, Loss: 0.1127\n",
            "Epoch [6/6], Step [310/340], Total Loss: 2.0518, Loss: 0.3131\n",
            "Epoch [6/6], Step [320/340], Total Loss: 1.5404, Loss: 0.1360\n",
            "Epoch [6/6], Step [330/340], Total Loss: 1.7020, Loss: 0.1487\n",
            "Epoch [6/6], Step [340/340], Total Loss: 1.4926, Loss: 0.1671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWDA4KwUPc4W"
      },
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, \"KGE_model_transE\")\n",
        "torch.save(model.state_dict(), \"KGE_model_transE_state\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udcMeeTEaeO0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}